remove >50%NA --> remove 7637 --> we now have 30190

we used 55 samples for training in brain, so for testing: 2 nigra only.


https://machinelearningmastery.com/better-naive-bayes/
- Use Probabilities For Feature Selection:
In Naive Bayes, the probabilities for each attribute are calculated independently from the training dataset. You can use a search algorithm to explore the combination of the probabilities of different attributes together and evaluate their performance at predicting the output variable.
- Remove Redundant Features:
Evaluate the correlation of attributes pairwise with each other using a correlation matrix and remove those features that are the most highly correlated.
Nevertheless, always test your problem before and after such a change and stick with the form of the problem that leads to the better results.
https://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/
- https://www.cs.waikato.ac.nz/ml/weka/downloading.html
formulas with mutual information:
https://en.wikipedia.org/wiki/Feature_selection#Minimum-redundancy-maximum-relevance_(mRMR)_feature_selection

